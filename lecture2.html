<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>reveal.js</title>
    
    <link rel="stylesheet" href="dist/reset.css">
    <link rel="stylesheet" href="dist/reveal.css">
    <link rel="stylesheet" href="dist/theme/adermon.css">
    
    <!-- <link rel="stylesheet" type="text/css" href="https://jsxgraph.uni-bayreuth.de/distrib/jsxgraph.css" /> -->
    <!-- <script type="text/javascript" src="https://jsxgraph.uni-bayreuth.de/distrib/jsxgraphcore.js"></script> -->
    <!-- <link href="./figures/jsxgraph/styles.css" rel="stylesheet" /> -->
    <!-- <link href="./figures/jsxgraph/shallow-nn-1d/style.css" rel="stylesheet" /> -->

    <link rel="stylesheet" href="styles.css"/>

    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="plugin/highlight/monokai.css">
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <h1>Machine Learning with Economic and Financial Applications</h1>
          <h4>Lecture 6: Natural language processing and LLMs</h4>
          <h5> Adrian Adermon</h5>
          <h5>2025-05-19</h5>
        </section>

        <section data-markdown>
          <textarea data-template>
            ## Natural language processing

            - Statistical models work with numbers
            
            - Text $\Rightarrow$ numerical data? 
            
            ---
            
            ## Bag-of-words
            Check each word against *dictionary*
            - One-hot encoding

            - Large vectors
            
            - Doesn't consider context or similarity
            
            ---
            
            ## Word embeddings

            *Word2vec*
            Simple neural network
            
            - Input: short sequences of text (e.g., five words) <!-- .element: class="fragment" data-fragment-index="1" -->
            
            - Self-supervised: <!-- .element: class="fragment" data-fragment-index="2" -->
              - Predict middle word (continuous bag of words) <!-- .element: class="fragment" data-fragment-index="2" -->
              - Predict surrounding words (skip-gram) <!-- .element: class="fragment" data-fragment-index="2" -->

            - Output: word embedding vectors <!-- .element: class="fragment" data-fragment-index="3" -->

            ---

            ## Semantic structure

            v(Paris) - v(France) + v(Italy) ≈ v(Rome)

            v(King) - v(Man) + v(Woman) ≈ v(Queen)
            
            ---
            
            ## Document as data
            
            ### Input
            
            Sequence of $L$ words: $\lbrace X_1, X_2, \ldots, X_L\rbrace$

            ### Output
            
            - Scalar (sentiment prediction)
            - Sequence (translation)

            ---

            ## Tokenization

            Split text into units from a *dictionary*

            Words as units?
            - Dictionary can't contain all possible words
            
            - Punctuation

            - Versions of words are related (e.g., walk, walks, walked, walking)

            ---

            ## Solution
            
            Split into *tokens*
            
            - Sometimes full words

            - Sometimes word fragments
            
          </Textarea>
        </section>

        <section>
          <h2>Recurrent neural network</h2>
          <img data-src="figures/typst/rnn.svg">
        </section>

        

        <section>
          <h2>Structure of an RNN</h2>
          Single hidden layer, $K$ hidden units
          
          $$A_{\ell k} = g \left ( w_{k0} + \sum_{j=1}^p w_{kj} X_{\ell j} + \sum_{s=1}^K u_{ks} A_{\ell-1,s} \right )$$

          $$O_\ell = \beta_0 + \sum_{k=1}^K \beta_k A_{\ell k}$$
        </section>

        <section>
          <h2>Recurrent neural network</h2>
          
        </section>

        

        <section>
          <h2>Challenges with text as data</h2>
          <blockquote style="font-size: 22pt; font-family: Cormorant">
            The restaurant refused to serve me a ham sandwich because it only cooks vegetarian food. In the end, they just gave me two slices of bread. The ambiance was just as good as the service.
          </blockquote>

          <ul>
            <li class="fragment"> Large input: 37 ⨉ 1024 = 37,888</li>
            <li class="fragment"> Input can differ in length</li>
            <li class="fragment"> Ambiguous syntax</li>
          </ul>
        </section>

        

        <section data-markdown>
          <textarea data-template>
            ## Attention
            "I swam across the river to get to the other bank"

            "I walked across the road to get cash from the bank"
          </textarea>
        </section>

        

        <section>
          <h2>Attention</h2>
          <img data-src="figures/typst/attention_example.svg">
          <ul>
            <li>Interpretation depends on context</li>
            <li>Attention is sequence-specific</li>
          </ul>
        </section>

        

        <section data-markdown>
          <textarea data-template>
            ## Input
            Sequence of *N* words $\lbrace x_n \rbrace$
            
            Each word represented as a *vector* of dimension *D*

            ## Output
            Sequence of *N* words $\lbrace y_n \rbrace$

            ---

            ## Attention

            Each output $y_n$ should depend on *all* inputs $x_1, \ldots, x_N$

            $$y_n = \sum_{m=1}^N a_{nm} x_m$$

            $a_{nm}$: *attention weights*
            - Non-negative
            - Sum to one

            ---

            ## Self-attention
            For each word $x_m$, define linear transformations
            - *Value*: word $x_n$
            - *Key*: relevant aspects of $x_n$
            - *Query*: relevant aspects of $x_m$

            For each $m$, weight *values* by similarity between *keys* and *query*

            Weights are scaled by *softmax* function

            $$a_{nm} = \frac{\exp(x_n \cdot x_m)}{\sum_{m^\prime=1}^N \exp(x_n \cdot x_{m^\prime})}$$


            ---

            ## Multi-head attention

            Use multiple attention *heads* in parallel
            - Identical structure, separate parameters
            - Learn different aspects of language

            ---
            
            ## Transformer layer
            Multi-head attention $\rightarrow$ by multilayer perceptron
            
            Stack multiple transformer layers

            ---

            ## Positional encoding

            Transformer *equivariant*:
            
            Permutation of inputs $\rightarrow$ same permutation of outputs

            But word order is important!

            RNN encode order in network architecture
            - Hard to parallelize

            ---
            Transformers encode order in *data*

            Add *positional* embedding vector to input vector: $\tilde{x}_n = x_n + r_n$

            ---

            ## Transformer language models (LLMs)
            - *Encoder*: text sequence $\rightarrow$ single output
            
            Example: sentiment analysis
            
            - *Decoder*: Single input $\rightarrow$ text sequence

            Example: text generation

            - *Encoder-decoder*: Sequence $\rightarrow$ sequence

            Example: machine translation

            ---
            
            ### Encoder
            Sequence $\rightarrow$ single output (e.g., sentiment analysis)
            
            ### Decoder <!-- .element: class="fragment" data-fragment-index="2" -->
            Single input $\rightarrow$ sequence (e.g., text generation) <!-- .element: class="fragment" data-fragment-index="2" -->

            ### Encoder-decoder <!-- .element: class="fragment" data-fragment-index="3" -->
            Sequence $\rightarrow$ sequence (e.g., machine translation) <!-- .element: class="fragment" data-fragment-index="3" -->

            ---
            
            ## Encoders
            Example: BERT (bidirectional encoder representations from transformers)

            1. Self-supervised pre-training: Predict masked words from sentences<!-- .element: class="fragment" data-fragment-index="2" -->

            2. Fine-tuning to specific task<!-- .element: class="fragment" data-fragment-index="3" -->
            - Text classification (e.g., sentiment analysis)<!-- .element: class="fragment" data-fragment-index="3" -->
            - Word classification (e.g., named entity recognition)<!-- .element: class="fragment" data-fragment-index="3" -->
            - Text span prediction (e.g., question answering)<!-- .element: class="fragment" data-fragment-index="3" -->
            
            ---

            ## Decoders

            Example: GPT3 (Generative Pretrained Transformer)

            Autoregressive language model
            <br>
                        
            ## Example<!-- .element: class="fragment" data-fragment-index="2" -->
            $$\Pr(\text{I am fine}) = \Pr(\text{I}) \times \Pr(\text{am} \mid \text{I}) \times \Pr(\text{fine} \mid \text {I am})$$<!-- .element: class="fragment" data-fragment-index="2" -->
            <br>

            ## General formulation<!-- .element: class="fragment" data-fragment-index="3" -->
            $$\Pr(t_1, t_2, \ldots, t_N) = \Pr(t_1) \prod_{n=2}^N \Pr(t_n \mid t_1, \ldots, t_{n-1})$$<!-- .element: class="fragment" data-fragment-index="3" -->

            ---
            ## Text generation with a decoder
            - Input: first *n-1* tokens
            - Output: predict *n<sup>th</sup>* token
            
            Repeat to predict token *n+1*

            ---

            ## Training a decoder

            - Pass whole sentence to transformer layer
            
            - Prevent cheating: *masked* self-attention
              - Set attention weights to -∞ for tokens to the right

          </textarea>
        </section>

        

        <section>
          <h2>Training a decoder</h2>
          <p>
            <span style="color: dodgerblue">Training input</span>
            →
            <span style="color: coral">Target value</span>
          </p>
          <div class="r-stack">
            <div class="fragment">
              "<span style="color: dodgerblue">I </span> <span style="color: coral">swam</span> <span style="color: gray">across the river to get to the other bank</span>"
            </div>
            
            <div class="fragment">
              <span style="color: dodgerblue">I swam </span> <span style="color: coral">across</span> <span style="color: gray">the river to get to the other bank</span>
            </div>
            
            <div class="fragment">
              <span style="color: dodgerblue">I swam across</span> <span style="color: coral">the</span> <span style="color: gray">river to get to the other bank</span>
            </div>
            
            <div class="fragment">
              <span style="color: dodgerblue">I swam across the</span> <span style="color: coral">river</span> <span style="color: gray">to get to the other bank</span>
            </div>
            
            <div class="fragment">
              <span style="color: dodgerblue">I swam across the river</span> <span style="color: coral">to</span> <span style="color: gray"> get to the other bank</span>
            </div>

            <div class="fragment">
              <span style="color: dodgerblue">I swam across the river to</span> <span style="color: coral">get</span> <span style="color: gray"> to the other bank</span>
            </div>
            
            <div class="fragment">
              <span style="color: dodgerblue">I swam across the river to get </span> <span style="color: coral">to</span> <span style="color: gray"> the other bank</span>
            </div>
            
            <div class="fragment">
              <span style="color: dodgerblue">I swam across the river to get to </span> <span style="color: coral">the</span> <span style="color: gray"> other bank</span>
            </div>
            
            <div class="fragment">
              <span style="color: dodgerblue">I swam across the river to get to the</span> <span style="color: coral">other</span> <span style="color: gray"> bank</span>
            </div>

            <div class="fragment">
              <span style="color: dodgerblue">I swam across the river to get to the other</span> <span style="color: coral">bank</span>
            </div>
          </div>
        </section>

        <section data-markdown>
          <textarea data-template>
            ## Structure of GPT3

            - Sequence length: 2,048 tokens

            - Batch size: 3.2m tokens

            - 96 transformer layers
            
            - 96 self-attention heads in each layer

            - Word embedding size: 12,228

            - Trained on 300 billion tokens

            - 175 billion parameters
            
            ---
            
            ## Encoder-decoder
            Machine translation
            - Input: sentence in English
            - Output: sentence in Swedish

            Training:
            - Masked self-attention on output
            - Full attention on input
          </textarea>
        </section>

            

        <section>
          <h2>Dot-product self-attention</h2>
          <img data-src="figures/typst/attention.svg" style="height: 45vh">
        </section>
        
        <section data-markdown>
          <textarea data-template>
            $$
            \begin{bmatrix} x_1 & x_2 & x_3 \end{bmatrix}
            \begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\\\ w_{31} & w_{32} \end{bmatrix} =
            \begin{bmatrix}
            \sum_{i=1}^3 x_i w_{i1} &
            \sum_{i=1}^3 x_i w_{i2} 
            \end{bmatrix}
            $$
          </textarea>
        </section>

        <section>
          <h2>Long-distance attention</h2>
          <img data-src="./figures/papers/attention_fig3.svg" style="width: 80vw">
        </section>
        
        <section>
          <h2>Two attention heads</h2>
          <img data-src="./figures/papers/attention_fig4.svg" style="height: 50vh; transform: rotate(90deg)">
        </section>
        
        <section>
          <h2>Heads specialize</h2>
          <img data-src="./figures/papers/attention_fig5.svg" style="height: 50vh; transform: rotate(90deg)">
        </section>

      </div>
    </div>

    <script src="dist/reveal.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script src="plugin/math/math.js"></script>
    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
	  hash: true,
	  // Learn about plugins: https://revealjs.com/plugins/
	  plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.MathJax3 ]
      });
    </script>
  </body>
</html>
